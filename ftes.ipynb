{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Khoa Tran - INFO 256 Fall 2013\n",
      "# Final Project Demo\n",
      "# Facebook Topics Extraction System"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Project Goals:\n",
      "\n",
      "* Identify popular topics in a given facebook page\n",
      "* Find similar posts related to a user's interest, and compare the results with facebook's"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Motivation\n",
      "\n",
      "* FB Search seems to be missing some results\n",
      "* See how well a simple TFIDF model would perform against FB Search\n",
      "* Finally... find out some popular topics being discussed without reading thru every post"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/fb_search_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Connects to Facebook    \n",
      "Login to your Facebook account and go to https://developers.facebook.com/tools/explorer/ to obtain and set permissions for an access token."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ACCESS_TOKEN = 'CAACEdEose0cBAMkOPptLZCUoAlRsUfD5nOnLcDo32ce6KZBKCxhOJ6E4YBjrQAFAxmoWmIdyd8AWl1TJoghswmnUKrTixUB5Toil9FsyfTy8YLjCfq292flRfYIJML0nzf3QwpR5nOnOGXLfntTlvriHnrXZBRQUChXMv3NPNbPTdeoKdgxDuXt7QCpfhVO8GCUXcNZAggZDZD'\n",
      "SEARCH_LIMIT = 500  # facebook allows 500 max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 294
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Import packages and dependencies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import facebook  # pip install facebook-sdk, not facebook\n",
      "import os\n",
      "import random\n",
      "\n",
      "# Plotting\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import networkx as nx\n",
      "%matplotlib inline\n",
      "\n",
      "# Making request, pretty print stuff\n",
      "import requests\n",
      "import json\n",
      "import simplejson as sj\n",
      "from prettytable import PrettyTable\n",
      "from collections import defaultdict, Counter\n",
      "\n",
      "# NLP!\n",
      "import string\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "import tagger as tag\n",
      "import enchant\n",
      "from nltk.metrics import edit_distance\n",
      "from pattern.vector import Document, Model, TFIDF, LEMMA, KMEANS, HIERARCHICAL, COSINE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 394
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Lemmatizer, stemmer, and spelling corrector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = nltk.WordNetLemmatizer()\n",
      "stemmer = nltk.stem.porter.PorterStemmer()\n",
      "table = string.maketrans(\"\", \"\")\n",
      "sw = stopwords.words('english')\n",
      "\n",
      "# Spelling corrector\n",
      "spell_dict = enchant.Dict('en')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 395
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helper functions for pretty printing, converting text to ascii, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pp(o):\n",
      "    '''\n",
      "    A helper function to pretty-print Python objects as JSON\n",
      "    '''\n",
      "    print json.dumps(o, indent=2)\n",
      "    \n",
      "def to_ascii(unicode_text):\n",
      "    '''\n",
      "    Converts unicode text to ascii. Also removes newline \\n \\r characters\n",
      "    '''\n",
      "    return unicode_text.encode('ascii', 'ignore').\\\n",
      "            replace('\\n', ' ').replace('\\r', '').strip()\n",
      "    \n",
      "def strip(s, punc=False):\n",
      "    '''\n",
      "    Strips punctuation and whitespace from a string\n",
      "    '''\n",
      "    if punc:\n",
      "        stripped = s.strip().translate(table, string.punctuation)\n",
      "        return ' '.join(stripped.split())\n",
      "    else:\n",
      "        return ' '.join(s.strip().split())\n",
      "\n",
      "def lower(word):\n",
      "    '''\n",
      "    Lowercases a word\n",
      "    '''\n",
      "    return word.lower()\n",
      "\n",
      "def lemmatize(word):\n",
      "    '''\n",
      "    Lemmatizes a word\n",
      "    '''\n",
      "    return lemmatizer.lemmatize(word)\n",
      "\n",
      "def stem(word):\n",
      "    '''\n",
      "    Stems a word using the Porter Stemmer\n",
      "    '''\n",
      "    return stemmer.stem_word(word)\n",
      "\n",
      "def correct(word, max_dist=2):\n",
      "    '''\n",
      "    Corrects spelling of a word. If the word is already correct,  cannot be corrected at all,\n",
      "    or the edit distance is greater than 2, just returns the input word\n",
      "    \n",
      "    If it can be corrected, finds a list of suggestions and \n",
      "    returns the first result whose edit distance doesn't exceed a threshold\n",
      "    \n",
      "    Edit distance: https://en.wikipedia.org/wiki/Edit_distance\n",
      "    '''\n",
      "    if spell_dict.check(word):\n",
      "        return word\n",
      "    \n",
      "    suggestions = spell_dict.suggest(word)\n",
      "    if suggestions and edit_distance(word, suggestions[0]) <= max_dist:\n",
      "        return suggestions[0]\n",
      "    return word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 358
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some tiny examples for error correction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Works\n",
      "print correct('aninal')\n",
      "print correct('behols'), '\\n'\n",
      "\n",
      "# Doesn't work... very well\n",
      "print correct('aisplane') # airplane\n",
      "print spell_dict.suggest('aisplane')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "animal\n",
        "behold"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "\n",
        "tailplane"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['tailplane', 'seaplane', 'ski-plane', 'biplane', 'esplanade', 'asplenium']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 354
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creates a connection to the Graph API with your access token"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = facebook.GraphAPI(ACCESS_TOKEN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 299
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part I: Retrieves a group's feed, and builds a simple search engine with TFIDF and Cosine Similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**To retrieve a group's feed, you first need to obtain the group's ID. To my knowledge, Facebook doesn't offer any easy way to do that. 'View Page Source' is one option, but I've found a couple of third-party \n",
      "services like http://wallflux.com/facebook_id/ is much easier to use**   \n",
      "\n",
      "The example below uses the **Berkeley CS Group** https://www.facebook.com/groups/berkeleycs/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Only needs to make connection once\n",
      "cal_cs_id = '266736903421190'\n",
      "cal_cs_feed = g.get_connections(cal_cs_id, 'feed', limit=SEARCH_LIMIT)['data']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "GraphAPIError",
       "evalue": "Error validating access token: Session has expired at unix time 1386752400. The current unix time is 1386758084.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mGraphAPIError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-443-09003d087db4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcal_cs_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'266736903421190'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcal_cs_feed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_connections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcal_cs_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEARCH_LIMIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcal_cs_feed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/khoatran/.virtualenvs/ftes/lib/python2.7/site-packages/facebook.pyc\u001b[0m in \u001b[0;36mget_connections\u001b[1;34m(self, id, connection_name, **args)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_connections\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;34m\"\"\"Fetchs the connections for given object.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconnection_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent_object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/khoatran/.virtualenvs/ftes/lib/python2.7/site-packages/facebook.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, path, args, post_args)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mGraphAPIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;31m# Timeout support for Python <2.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mGraphAPIError\u001b[0m: Error validating access token: Session has expired at unix time 1386752400. The current unix time is 1386758084."
       ]
      }
     ],
     "prompt_number": 443
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pp(cal_cs_feed[15])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(cal_cs_feed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 301,
       "text": [
        "493"
       ]
      }
     ],
     "prompt_number": 301
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_feed(feed):\n",
      "    '''\n",
      "    Prints out every post, along with its comments, in a feed\n",
      "    '''\n",
      "    for post in feed:\n",
      "        if 'message' in post:\n",
      "            msg = strip(to_ascii(post['message']))\n",
      "            print 'POST:', msg, '\\n'\n",
      "        \n",
      "        print 'COMMENTS:'\n",
      "        if 'comments' in post:\n",
      "            for comment in post['comments']['data']:\n",
      "                if 'message' in comment:\n",
      "                    comment = strip(to_ascii(comment['message']))\n",
      "                    if comment is not None and comment != '':\n",
      "                        print '+', comment\n",
      "        print '-----------------------------------------\\n'\n",
      "        \n",
      "print_feed(cal_cs_feed[10:12])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "POST: http://www.lrb.co.uk/v35/n03/rebecca-solnit/diary I think it's important to think about the social implications of science and technology, especially to those who are or will be involved in it (like us). In the attached article, Rebecca Solnit discusses the tech boom in Silicon Valley and what it means for those who benefit from it and those who don't. Might not be the shortest read, but it's well worth it. \n",
        "\n",
        "COMMENTS:\n",
        "-----------------------------------------\n",
        "\n",
        "POST: CS 191C PEOPLE: Joint CS Theory-Condensed Matter Seminar Title: Taming a quantum tiger Speaker: Umesh Vazirani, U.C. Berkeley 2:30 pm Monday Dec 9, Banatao Auditorium, Sutardja Dai Hall Abstract: Quantum computation has taught us that a quantum system is better visualized as a wild tiger rather than a tame Schrodinger's cat. Indeed the exponential power of quantum systems, which makes quantum computers possible, is also an enormous obstacle to analyzing and understanding physical systems. Are there situations in which we can neutralize this curse of exponentially? In this talk I will address this question in two settings: 1. Are there general classes of quantum systems whose states can be efficiently (polynomially) computed on a traditional classical computer? And can we explain the remarkable practical success of the heuristic DMRG for 1D systems? 2. Is it possible for a classical experimentalist to test that an untrusted quantum system behaves as commanded? For example, is it possible to test that a claimed quantum computer is really quantum? The talk will discuss recent breakthroughs in both settings, as well as many exciting open questions. It will also briefly touch upon recent results that shed some light on the D-Wave controversy. The talk may also be thought of as an \"amuse bouche\" for the upcoming semester-long program on Quantum Hamiltonian Complexity at the Simons Institute, and I will make every attempt to keep the discussion widely accessible. Based on joint work with Itai Arad, Alexei Kitaev, Zeph Landau and Thomas Vidick, Ben Reichardt and Falk Unger, and Seung Woo Shin. \n",
        "\n",
        "COMMENTS:\n",
        "+ noooooooooo this is during my review session, is this going to be recorded?\n",
        "+ I hope I recorded most of it. I'll see about figuring out how to get it off my phone and up online somewhere (I lost power for a few minutes, about 10 minutes in, so I need to figure out how to splice together 2 videos as well first).\n",
        "+ Also, currently I've NO idea how good the sound/picture/stability quality is yet until I can see it on a full screen, haha.\n",
        "-----------------------------------------\n",
        "\n",
        "POST: What's your opinion on the machine learning class offered in the Stats department, STAT 154? I've already taken CS188 but now I'm deciding whether to take CS189 or Stat 154 or both. I just want to know the overlap between the 2 classes, if there is any. \n",
        "\n",
        "COMMENTS:\n",
        "+ not sure about the actual content, but 154 has a (reportedly) bad prof while 189 has a decent prof.\n",
        "-----------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 441
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_link(post):\n",
      "    '''\n",
      "    Finds the permanent link to a given post\n",
      "    '''\n",
      "    if 'actions' in post:\n",
      "        actions = post['actions']\n",
      "        for action in actions:\n",
      "            if 'link' in action:\n",
      "                return action['link']\n",
      "    return ''\n",
      "    \n",
      "def save_feed(feed):\n",
      "    '''\n",
      "    Saves the input feed in a Python list for later processing\n",
      "    Also strips whitespace and lemmatizes along the way\n",
      "    '''\n",
      "    posts = []\n",
      "    for post in feed:\n",
      "        if 'message' in post and 'actions' in post:\n",
      "            msg = strip(to_ascii(post['message']))\n",
      "            link = strip(to_ascii(find_link(post)))\n",
      "            posts.append((msg, link))\n",
      "        \n",
      "        if 'comments' in post:\n",
      "            for comment in post['comments']['data']:\n",
      "                if 'message' in comment and 'actions' in comment:\n",
      "                    msg = strip(to_ascii(comment['message']))\n",
      "                    link = strip(to_ascii(find_link(comment)))\n",
      "                    if msg is not None and msg != '':\n",
      "                        posts.append((msg, link))\n",
      "    return posts\n",
      "                \n",
      "feed = save_feed(cal_cs_feed)\n",
      "feed[30:35]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 442,
       "text": [
        "[(\"I have an ASUS S400CA laptop, I tried to upgrade the RAM but can't get the computer to recognize it, even though I have flashed my bios to the newest version for my computer (209). Can anyone enlighten me on this matter?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/565004033594474'),\n",
        " (\"easy upper div cs class that's not 188, 160, 161, or 169?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/564782570283287'),\n",
        " (\"Hey all, I'm a junior transfer entering my second semester here at Cal, LnS CS. I've completed all pre-requisites outside of CS61C (and EE20 if you want to count it). I'm enrolled in CS61C and Stat134 next semester. I'm waitlist ~#300 in CS170 and ~#170 in CS164. I really don't have any other classes to take outside of upper division courses, but it seems like they are all full. Should I stick it out with these courses, or should I just start looking for anything that could be open? Any recommendations?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/564577966970414'),\n",
        " ('for the cs major planning worksheet, am I supposed to start filling in the classes that I am currently taking right now? or next semester?',\n",
        "  'https://www.facebook.com/266736903421190/posts/565153826912828'),\n",
        " (\"Does anyone know how the CS70 waitlist is processed? There are 16 people enrolled in my section and I'm #6 on my waitlist. it says the waitlist is processed manually\",\n",
        "  'https://www.facebook.com/266736903421190/posts/564188787009332')]"
       ]
      }
     ],
     "prompt_number": 442
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words_tfidf(lst):\n",
      "    '''\n",
      "    Constructs a bag of words model, where each document is a Facebook post/comment\n",
      "    Also applies TFIDF weighting, lemmatization, and filter out stopwords\n",
      "    '''\n",
      "    model = Model(documents=[], weight=TFIDF)\n",
      "    for msg, link in lst:\n",
      "        doc = Document(msg, stemmer=LEMMA, stopwords=True, name=msg, description=link)\n",
      "        model.append(doc)\n",
      "    return model\n",
      "\n",
      "def cosine_similarity(model, term, num=10):\n",
      "    '''\n",
      "    Finds the cosine similarity between the input document and each document in \n",
      "    the corpus, and outputs the best 'num' results\n",
      "    '''\n",
      "    doc = Document(term, stemmer=LEMMA, stopwords=True, name=term)\n",
      "    return model.neighbors(doc, top=num)\n",
      "\n",
      "def process_similarity(result):\n",
      "    '''\n",
      "    Processes the result in a nicely formatted table\n",
      "    \n",
      "    result is a tuple of length 2, where the first item is the similarity score, \n",
      "    and the second item is the document itself\n",
      "    '''\n",
      "    pt = PrettyTable(field_names=['Post', 'Sim', 'Link'])\n",
      "    pt.align['Post'], pt.align['Sim'], pt.align['Link'] = 'l', 'l', 'l'\n",
      "    [ pt.add_row([res[1].name[:45] + '...', \"{0:.2f}\".format(res[0]), \n",
      "                  res[1].description]) for res in result ]\n",
      "    return pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 304
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Constructs the bag of words model.\n",
      "# We don't need to call this function more than once, unless the corpus changed\n",
      "bag_of_words = bag_of_words_tfidf(feed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 305
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Enter your query below, along with the number of results you want to search for"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "QUERY = 'declaring major early'\n",
      "NUM_SEARCH = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 306
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sim = cosine_similarity(bag_of_words, QUERY, NUM_SEARCH)\n",
      "print process_similarity(sim)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n",
        "| Post                                             | Sim  | Link                                                           |\n",
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n",
        "| What are the requirements for declaring the C... | 0.82 | https://www.facebook.com/266736903421190/posts/531887326906145 |\n",
        "| In terms of petitioning for the major, I had ... | 0.23 | https://www.facebook.com/266736903421190/posts/554112774683600 |\n",
        "| Hey guys, I'm an intended LSCS major. The LS-... | 0.23 | https://www.facebook.com/266736903421190/posts/537636692997875 |\n",
        "| Is it true that LnS CS majors are relatively ... | 0.23 | https://www.facebook.com/266736903421190/posts/534359746658903 |\n",
        "| Just for clarification... to declare early (w... | 0.17 | https://www.facebook.com/266736903421190/posts/554103854684492 |\n",
        "| Hello CS Majors, Here's an updated graphic of... | 0.17 | https://www.facebook.com/266736903421190/posts/557605601000984 |\n",
        "| I am currently a freshman and was trying to m... | 0.16 | https://www.facebook.com/266736903421190/posts/538359636258914 |\n",
        "| with the new cs major requirement...cs162 is ... | 0.16 | https://www.facebook.com/266736903421190/posts/554548241306720 |\n",
        "| So as an undeclared CS major finishing my pre... | 0.12 | https://www.facebook.com/266736903421190/posts/544901308938080 |\n",
        "| hey guys! so i am a cog sci major cs minor, d... | 0.12 | https://www.facebook.com/266736903421190/posts/535700736524804 |\n",
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n"
       ]
      }
     ],
     "prompt_number": 307
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## compares with"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/fb_search_0.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**, which I think is not bad at all. The top result from this search system is:**    \n",
      "![](files/img/fb_search_2.png)\n",
      "\n",
      ", **whereas the top result for FB search is:**    \n",
      "![](files/img/fb_search_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part II: What are the most popular topics in a group's feed right now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
      "      ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.                # ellipsis\n",
      "    | [][.,;\"'?():-_`]      # these are separate tokens\n",
      "'''\n",
      "\n",
      "# Noun phrase chunker\n",
      "grammar = r\"\"\"\n",
      "    # Nouns and Adjectives, terminated with Nouns\n",
      "    NBAR:\n",
      "        {<NN.*|JJ>*<NN.*>}\n",
      "        \n",
      "    # Above, connected with preposition or subordinating conjunction (in, of, etc...)\n",
      "    NP:\n",
      "        {<NBAR>}\n",
      "        {<NBAR><IN><NBAR>}\"\"\"\n",
      "chunker = nltk.RegexpParser(grammar)\n",
      "\n",
      "# POS tagger\n",
      "tagger = tag.tagger()\n",
      "\n",
      "def leaves(tree):\n",
      "    '''\n",
      "    Finds NP (nounphrase) leaf nodes of a chunk tree\n",
      "    '''\n",
      "    for subtree in tree.subtrees(filter = lambda t: t.node=='NP'):\n",
      "        yield subtree.leaves()\n",
      "\n",
      "def normalize(word):\n",
      "    '''\n",
      "    Normalizes words to lowercase and stems/lemmatizes it\n",
      "    '''\n",
      "    word = word.lower()\n",
      "    #word = stem(word)\n",
      "    word = strip(lemmatize(word), True)\n",
      "    return word\n",
      "\n",
      "def acceptable_word(word):\n",
      "    '''\n",
      "    Checks conditions for acceptable word: valid length and no stopwords\n",
      "    '''\n",
      "    accepted = bool(2 <= len(word) <= 40\n",
      "        and word.lower() not in sw)\n",
      "    return accepted\n",
      "\n",
      "def get_terms(tree):\n",
      "    '''\n",
      "    Gets all the acceptable noun_phrase term from the syntax tree\n",
      "    '''\n",
      "    for leaf in leaves(tree):\n",
      "        term = [normalize(w) for w, t in leaf if acceptable_word(w)]\n",
      "        yield term\n",
      "\n",
      "def extract_noun_phrases(text):\n",
      "    '''\n",
      "    Extracts all noun_phrases from a given text\n",
      "    '''\n",
      "    toks = nltk.regexp_tokenize(text, sentence_re)\n",
      "    postoks = tagger.tag(toks)\n",
      "\n",
      "    # Builds a POS tree\n",
      "    tree = chunker.parse(postoks)\n",
      "    terms = get_terms(tree)\n",
      "\n",
      "    # Extracts Noun Phrase\n",
      "    noun_phrases = []\n",
      "    for term in terms:\n",
      "        np = \"\"\n",
      "        for word in term:\n",
      "            np += word + \" \"\n",
      "        if np != \"\":\n",
      "            noun_phrases.append(np.strip())\n",
      "    return noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully loaded POS tagger\n"
       ]
      }
     ],
     "prompt_number": 424
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feed(feed):\n",
      "    '''\n",
      "    Extracts popular topics (noun phrases) from a feed, and builds a simple\n",
      "    counter to keep track of the popularity\n",
      "    '''\n",
      "    topics = defaultdict(int)\n",
      "    for post, link in feed:\n",
      "        noun_phrases = extract_noun_phrases(post)\n",
      "        for np in noun_phrases:\n",
      "            topics[np] += 1\n",
      "    return topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 425
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topics = extract_feed(feed)\n",
      "c = Counter(topics)\n",
      "c.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 436,
       "text": [
        "[('http', 90),\n",
        " ('class', 65),\n",
        " ('www', 63),\n",
        " ('semester', 63),\n",
        " ('c', 52),\n",
        " ('facebook', 41),\n",
        " ('phase', 40),\n",
        " ('ve', 38),\n",
        " ('course', 33),\n",
        " ('student', 30),\n",
        " ('guy', 29),\n",
        " ('re', 28),\n",
        " ('thanks', 26),\n",
        " ('61b', 25),\n",
        " ('math', 24),\n",
        " ('ee20', 21),\n",
        " ('people', 20),\n",
        " ('cs61b', 20),\n",
        " ('telebears', 18),\n",
        " ('question', 18)]"
       ]
      }
     ],
     "prompt_number": 436
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pytagcloud import create_tag_image, create_html_data, make_tags, LAYOUT_HORIZONTAL, LAYOUTS\n",
      "from pytagcloud.colors import COLOR_SCHEMES\n",
      "from operator import itemgetter\n",
      "\n",
      "def get_tag_counts(counter):\n",
      "    '''\n",
      "    Get the noun phrase counts for word cloud by first converting the counter to a dict\n",
      "    '''\n",
      "    return sorted(dict(counter).iteritems(), key=itemgetter(1), reverse=True)\n",
      "    \n",
      "def create_cloud(counter):\n",
      "    '''\n",
      "    Creates a word cloud from a counter\n",
      "    '''\n",
      "    tags = make_tags(get_tag_counts(counter)[:80], maxsize=120, \n",
      "                     colors=COLOR_SCHEMES['goldfish'])\n",
      "    create_tag_image(tags, './img/cloud_large.png', \n",
      "                     size=(900, 600), background=(0, 0, 0, 255), \n",
      "                     layout=LAYOUT_HORIZONTAL, fontname='Lobster')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 434
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_cloud(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 435
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/cloud_large.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Results:\n",
      "\n",
      "Part I:\n",
      "\n",
      "* Tried out at least 5 different queries with the simple TFIDF search, and the result seems to be on par with FB Search. The ranking, as seen above, is different, however.      \n",
      "\n",
      "* Revisit and try out different algorithms, including stemming, typo correction, variants of TFIDF (maxmimum TF normalization, sublinear scaling, etc.) http://nlp.stanford.edu/IR-book/html/htmledition/variant-tf-idf-functions-1.html\n",
      "\n",
      "Part II:\n",
      "\n",
      "* There are quite some noises ('http', 'www', etc.) and words that don't really tell you anything new ('class', 'course', etc.)    \n",
      "\n",
      "* The overall result seems consistent with what most CS students usually discuss on FB ('telebears', '61B', etc.)\n",
      "\n",
      "Overall a great project to work on and extend in the future!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Thanks!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}