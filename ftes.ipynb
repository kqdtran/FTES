{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Khoa Tran - INFO 256 Fall 2013\n",
      "# Final Project Demo\n",
      "# Facebook Topics Extraction System"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Link to Final Project Report: https://www.dropbox.com/s/53g8z9do5ciy4of/report.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Project Goals:\n",
      "\n",
      "* Identify popular topics in a given facebook page\n",
      "* Find similar posts related to a user's interest, and compare the results with facebook's"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Motivation\n",
      "\n",
      "* FB Search seems to be missing some results\n",
      "* See how well a simple TFIDF model would perform against FB Search\n",
      "* Finally... find out some popular topics being discussed without reading thru every post"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Why Facebook?\n",
      "\n",
      "* Hasn't been explored that much compared to Twitter, mostly due to the complexity (~~and bad documentation~~) of the API\n",
      "\n",
      "* No 140-character limit like Twitter => more full, grammatically-correct words for NLP analysis\n",
      "\n",
      "* Very little support for Python (official languages include JS, PHP, among a couple others) => great problem to crack\n",
      "\n",
      "* Finally... (I) never did this before => should be interesting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Connects to Facebook    \n",
      "Login to your Facebook account and go to https://developers.facebook.com/tools/explorer/ to obtain and set permissions for an access token."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ACCESS_TOKEN = ''\n",
      "SEARCH_LIMIT = 500  # facebook allows 500 max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Import packages and dependencies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import facebook  # pip install facebook-sdk, not facebook\n",
      "import os\n",
      "import random\n",
      "\n",
      "# Plotting\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import networkx as nx\n",
      "%matplotlib inline\n",
      "\n",
      "# Making request, pretty print stuff\n",
      "import requests\n",
      "import json\n",
      "import simplejson as sj\n",
      "from prettytable import PrettyTable\n",
      "from collections import defaultdict, Counter\n",
      "\n",
      "# NLP!\n",
      "import string\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "import tagger as tag\n",
      "from nltk.metrics import edit_distance\n",
      "from pattern.vector import Document, Model, TFIDF, LEMMA, KMEANS, HIERARCHICAL, COSINE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Lemmatizer, stemmer, and spelling corrector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = nltk.WordNetLemmatizer()\n",
      "stemmer = nltk.stem.porter.PorterStemmer()\n",
      "table = string.maketrans(\"\", \"\")\n",
      "sw = stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Helper functions for pretty printing, converting text to ascii, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pp(o):\n",
      "    '''\n",
      "    A helper function to pretty-print Python objects as JSON\n",
      "    '''\n",
      "    print json.dumps(o, indent=2)\n",
      "    \n",
      "def to_ascii(unicode_text):\n",
      "    '''\n",
      "    Converts unicode text to ascii. Also removes newline \\n \\r characters\n",
      "    '''\n",
      "    return unicode_text.encode('ascii', 'ignore').\\\n",
      "            replace('\\n', ' ').replace('\\r', '').strip()\n",
      "    \n",
      "def strip(s, punc=False):\n",
      "    '''\n",
      "    Strips punctuation and whitespace from a string\n",
      "    '''\n",
      "    if punc:\n",
      "        stripped = s.strip().translate(table, string.punctuation)\n",
      "        return ' '.join(stripped.split())\n",
      "    else:\n",
      "        return ' '.join(s.strip().split())\n",
      "\n",
      "def lower(word):\n",
      "    '''\n",
      "    Lowercases a word\n",
      "    '''\n",
      "    return word.lower()\n",
      "\n",
      "def lemmatize(word):\n",
      "    '''\n",
      "    Lemmatizes a word\n",
      "    '''\n",
      "    return lemmatizer.lemmatize(word)\n",
      "\n",
      "def stem(word):\n",
      "    '''\n",
      "    Stems a word using the Porter Stemmer\n",
      "    '''\n",
      "    return stemmer.stem_word(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Spelling correction\n",
      "(from Peter Norvig's famous blog post http://norvig.com/spell-correct.html)   \n",
      "\n",
      "This was tested out, but the final result below does not involve spelling correction since the final search result score seems to decrease."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re, collections\n",
      "\n",
      "def words(text): return re.findall('[a-z]+', text.lower()) \n",
      "\n",
      "def train(features):\n",
      "    model = collections.defaultdict(lambda: 1)\n",
      "    for f in features:\n",
      "        model[f] += 1\n",
      "    return model\n",
      "\n",
      "NWORDS = train(words(file('big.txt').read()))\n",
      "\n",
      "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "\n",
      "def edits1(word):\n",
      "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
      "   deletes    = [a + b[1:] for a, b in splits if b]\n",
      "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
      "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
      "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
      "   return set(deletes + transposes + replaces + inserts)\n",
      "\n",
      "def known_edits2(word):\n",
      "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
      "\n",
      "def known(words): return set(w for w in words if w in NWORDS)\n",
      "\n",
      "def correct(word):\n",
      "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
      "    return max(candidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creates a connection to the Graph API with your access token"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = facebook.GraphAPI(ACCESS_TOKEN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part I: Retrieves a group's feed, and builds a simple search engine with TFIDF and Cosine Similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**To retrieve a group's feed, you first need to obtain the group's ID. To my knowledge, Facebook doesn't offer any easy way to do that. 'View Page Source' is one option, but I've found a couple of third-party \n",
      "services like http://wallflux.com/facebook_id/ is much easier to use**   \n",
      "\n",
      "The example below uses the **Berkeley CS Group** https://www.facebook.com/groups/berkeleycs/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Only needs to make connection once\n",
      "cal_cs_id = '266736903421190'\n",
      "cal_cs_feed = g.get_connections(cal_cs_id, 'feed', limit=SEARCH_LIMIT)['data']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pp(cal_cs_feed[15])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        "  \"picture\": \"https://fbexternal-a.akamaihd.net/safe_image.php?d=AQALS2YIM9l4s9-y&w=154&h=154&url=http%3A%2F%2Flh3.googleusercontent.com%2Fuy3HQlPPel_qsIn6G8Za5SgyJBD6JIRx-uIEEFgF0XGbyHYeJQBPSSDpC4-wj-Ttdvg\", \n",
        "  \"from\": {\n",
        "    \"name\": \"Arjun Ghai\", \n",
        "    \"id\": \"543681184\"\n",
        "  }, \n",
        "  \"name\": \"CETSA Membership Form\", \n",
        "  \"caption\": \"docs.google.com\", \n",
        "  \"privacy\": {\n",
        "    \"value\": \"\"\n",
        "  }, \n",
        "  \"actions\": [\n",
        "    {\n",
        "      \"link\": \"https://www.facebook.com/266736903421190/posts/567231850038359\", \n",
        "      \"name\": \"Comment\"\n",
        "    }, \n",
        "    {\n",
        "      \"link\": \"https://www.facebook.com/266736903421190/posts/567231850038359\", \n",
        "      \"name\": \"Like\"\n",
        "    }\n",
        "  ], \n",
        "  \"updated_time\": \"2013-12-08T22:48:10+0000\", \n",
        "  \"to\": {\n",
        "    \"data\": [\n",
        "      {\n",
        "        \"name\": \"Computer Science\", \n",
        "        \"id\": \"266736903421190\"\n",
        "      }\n",
        "    ]\n",
        "  }, \n",
        "  \"link\": \"https://docs.google.com/forms/d/1fb5lr77I0lLrtZuMmfuO_GWCMyQ-HxmZ7DMCf0FjLjo/viewform\", \n",
        "  \"likes\": {\n",
        "    \"paging\": {\n",
        "      \"cursors\": {\n",
        "        \"after\": \"NjI4NjQwMTMz\", \n",
        "        \"before\": \"MTAwMDAwMjY2NjUyNTE5\"\n",
        "      }\n",
        "    }, \n",
        "    \"data\": [\n",
        "      {\n",
        "        \"id\": \"100000266652519\", \n",
        "        \"name\": \"Sebastian Edward Shanus\"\n",
        "      }, \n",
        "      {\n",
        "        \"id\": \"628640133\", \n",
        "        \"name\": \"Pavan Patel\"\n",
        "      }\n",
        "    ]\n",
        "  }, \n",
        "  \"created_time\": \"2013-12-08T22:48:10+0000\", \n",
        "  \"message\": \"Looking to put the hours you have spent understanding new material to good use and maybe make some money out of it. The Center for Entrepreneurship and Technology Student Association (CETSA) connects students to large corporations based here in the Bay Area. Fill out the membership form and see how CETSA can help grow your network and land you a job/internship. https://docs.google.com/forms/d/1fb5lr77I0lLrtZuMmfuO_GWCMyQ-HxmZ7DMCf0FjLjo/viewform\", \n",
        "  \"icon\": \"https://fbstatic-a.akamaihd.net/rsrc.php/v2/yD/r/aS8ecmYRys0.gif\", \n",
        "  \"type\": \"link\", \n",
        "  \"id\": \"266736903421190_567231850038359\", \n",
        "  \"description\": \"By signing up for CETSA, you will gain exclusive access to our internal network and tech entities such as Pandora, Yelp, Andreessen Horowitz, Spoon Rocket, and SkyDeck.\"\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(cal_cs_feed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "494"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_feed(feed):\n",
      "    '''\n",
      "    Prints out every post, along with its comments, in a feed\n",
      "    '''\n",
      "    for post in feed:\n",
      "        if 'message' in post:\n",
      "            msg = strip(to_ascii(post['message']))\n",
      "            print 'POST:', msg, '\\n'\n",
      "        \n",
      "        print 'COMMENTS:'\n",
      "        if 'comments' in post:\n",
      "            for comment in post['comments']['data']:\n",
      "                if 'message' in comment:\n",
      "                    comment = strip(to_ascii(comment['message']))\n",
      "                    if comment is not None and comment != '':\n",
      "                        print '+', comment\n",
      "        print '-----------------------------------------\\n'\n",
      "        \n",
      "print_feed(cal_cs_feed[10:12])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "POST: Has anyone taken Stats 133? What is it like? \n",
        "\n",
        "COMMENTS:\n",
        "+ Nope.\n",
        "+ I took it. Waste of time for a cs major. Prepare to learn nothing new\n",
        "+ Hmmm thanks then haha\n",
        "-----------------------------------------\n",
        "\n",
        "POST: http://www.lrb.co.uk/v35/n03/rebecca-solnit/diary I think it's important to think about the social implications of science and technology, especially to those who are or will be involved in it (like us). In the attached article, Rebecca Solnit discusses the tech boom in Silicon Valley and what it means for those who benefit from it and those who don't. Might not be the shortest read, but it's well worth it. \n",
        "\n",
        "COMMENTS:\n",
        "-----------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_link(post):\n",
      "    '''\n",
      "    Finds the permanent link to a given post\n",
      "    '''\n",
      "    if 'actions' in post:\n",
      "        actions = post['actions']\n",
      "        for action in actions:\n",
      "            if 'link' in action:\n",
      "                return action['link']\n",
      "    return ''\n",
      "    \n",
      "def save_feed(feed):\n",
      "    '''\n",
      "    Saves the input feed in a Python list for later processing\n",
      "    Also strips whitespace and lemmatizes along the way\n",
      "    '''\n",
      "    posts = []\n",
      "    for post in feed:\n",
      "        if 'message' in post and 'actions' in post:\n",
      "            msg = strip(to_ascii(post['message']))\n",
      "            link = strip(to_ascii(find_link(post)))\n",
      "            posts.append((msg, link))\n",
      "        \n",
      "        if 'comments' in post:\n",
      "            for comment in post['comments']['data']:\n",
      "                if 'message' in comment and 'actions' in comment:\n",
      "                    msg = strip(to_ascii(comment['message']))\n",
      "                    link = strip(to_ascii(find_link(comment)))\n",
      "                    if msg is not None and msg != '':\n",
      "                        posts.append((msg, link))\n",
      "    return posts\n",
      "                \n",
      "feed = save_feed(cal_cs_feed)\n",
      "feed[30:35]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[(\"Pinching and zooming isn't the greatest for mobile but why is it that very few responsive websites allow you to zoom or change the font size? Really this seems like a big oversight /soapbox Are there any mobile frameworks that focus on good accessibility?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/565404913554386'),\n",
        " (\"I have an ASUS S400CA laptop, I tried to upgrade the RAM but can't get the computer to recognize it, even though I have flashed my bios to the newest version for my computer (209). Can anyone enlighten me on this matter?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/565004033594474'),\n",
        " (\"easy upper div cs class that's not 188, 160, 161, or 169?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/564782570283287'),\n",
        " (\"Hey all, I'm a junior transfer entering my second semester here at Cal, LnS CS. I've completed all pre-requisites outside of CS61C (and EE20 if you want to count it). I'm enrolled in CS61C and Stat134 next semester. I'm waitlist ~#300 in CS170 and ~#170 in CS164. I really don't have any other classes to take outside of upper division courses, but it seems like they are all full. Should I stick it out with these courses, or should I just start looking for anything that could be open? Any recommendations?\",\n",
        "  'https://www.facebook.com/266736903421190/posts/564577966970414'),\n",
        " ('for the cs major planning worksheet, am I supposed to start filling in the classes that I am currently taking right now? or next semester?',\n",
        "  'https://www.facebook.com/266736903421190/posts/565153826912828')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words_tfidf(lst):\n",
      "    '''\n",
      "    Constructs a bag of words model, where each document is a Facebook post/comment\n",
      "    Also applies TFIDF weighting, lemmatization, and filter out stopwords\n",
      "    '''\n",
      "    model = Model(documents=[], weight=TFIDF)\n",
      "    for msg, link in lst:\n",
      "        doc = Document(msg, stemmer=LEMMA, stopwords=True, name=msg, description=link)\n",
      "        model.append(doc)\n",
      "    return model\n",
      "\n",
      "def cosine_similarity(model, term, num=10):\n",
      "    '''\n",
      "    Finds the cosine similarity between the input document and each document in \n",
      "    the corpus, and outputs the best 'num' results\n",
      "    '''\n",
      "    doc = Document(term, stemmer=LEMMA, stopwords=True, name=term)\n",
      "    return model.neighbors(doc, top=num)\n",
      "\n",
      "def process_similarity(result):\n",
      "    '''\n",
      "    Processes the result in a nicely formatted table\n",
      "    \n",
      "    result is a tuple of length 2, where the first item is the similarity score, \n",
      "    and the second item is the document itself\n",
      "    '''\n",
      "    pt = PrettyTable(field_names=['Post', 'Sim', 'Link'])\n",
      "    pt.align['Post'], pt.align['Sim'], pt.align['Link'] = 'l', 'l', 'l'\n",
      "    [ pt.add_row([res[1].name[:45] + '...', \"{0:.2f}\".format(res[0]), \n",
      "                  res[1].description]) for res in result ]\n",
      "    return pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Constructs the bag of words model.\n",
      "# We don't need to call this function more than once, unless the corpus changed\n",
      "bag_of_words = bag_of_words_tfidf(feed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Enter your query below, along with the number of results you want to search for"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "QUERY = 'declaring major early'\n",
      "NUM_SEARCH = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sim = cosine_similarity(bag_of_words, QUERY, NUM_SEARCH)\n",
      "print process_similarity(sim)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n",
        "| Post                                             | Sim  | Link                                                           |\n",
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n",
        "| What are the requirements for declaring the C... | 0.82 | https://www.facebook.com/266736903421190/posts/531887326906145 |\n",
        "| In terms of petitioning for the major, I had ... | 0.23 | https://www.facebook.com/266736903421190/posts/554112774683600 |\n",
        "| Hey guys, I'm an intended LSCS major. The LS-... | 0.23 | https://www.facebook.com/266736903421190/posts/537636692997875 |\n",
        "| Is it true that LnS CS majors are relatively ... | 0.23 | https://www.facebook.com/266736903421190/posts/534359746658903 |\n",
        "| Just for clarification... to declare early (w... | 0.17 | https://www.facebook.com/266736903421190/posts/554103854684492 |\n",
        "| Hello CS Majors, Here's an updated graphic of... | 0.17 | https://www.facebook.com/266736903421190/posts/557605601000984 |\n",
        "| I am currently a freshman and was trying to m... | 0.16 | https://www.facebook.com/266736903421190/posts/538359636258914 |\n",
        "| with the new cs major requirement...cs162 is ... | 0.16 | https://www.facebook.com/266736903421190/posts/554548241306720 |\n",
        "| So as an undeclared CS major finishing my pre... | 0.12 | https://www.facebook.com/266736903421190/posts/544901308938080 |\n",
        "| hey guys! so i am a cog sci major cs minor, d... | 0.12 | https://www.facebook.com/266736903421190/posts/535700736524804 |\n",
        "+--------------------------------------------------+------+----------------------------------------------------------------+\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## compares with"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/fb_search_0.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**, which I think is not bad at all. The top result from this search system is:**    \n",
      "![](files/img/fb_search_2.png)\n",
      "\n",
      ", **whereas the top result for FB search is:**    \n",
      "![](files/img/fb_search_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part II: What are the most popular topics in a group's feed right now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Adapted and modified from https://gist.github.com/alexbowe/879414\n",
      "\n",
      "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
      "      ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.                # ellipsis\n",
      "    | [][.,;\"'?():-_`]      # these are separate tokens\n",
      "'''\n",
      "\n",
      "# Noun phrase chunker\n",
      "grammar = r\"\"\"\n",
      "    # Nouns and Adjectives, terminated with Nouns\n",
      "    NBAR:\n",
      "        {<NN.*|JJ>*<NN.*>}\n",
      "        \n",
      "    # Above, connected with preposition or subordinating conjunction (in, of, etc...)\n",
      "    NP:\n",
      "        {<NBAR>}\n",
      "        {<NBAR><IN><NBAR>}\"\"\"\n",
      "chunker = nltk.RegexpParser(grammar)\n",
      "\n",
      "# POS tagger - see tagger.py\n",
      "tagger = tag.tagger()\n",
      "\n",
      "def leaves(tree):\n",
      "    '''\n",
      "    Finds NP (nounphrase) leaf nodes of a chunk tree\n",
      "    '''\n",
      "    for subtree in tree.subtrees(filter = lambda t: t.node=='NP'):\n",
      "        yield subtree.leaves()\n",
      "\n",
      "def normalize(word):\n",
      "    '''\n",
      "    Normalizes words to lowercase and stems/lemmatizes it\n",
      "    '''\n",
      "    word = word.lower()\n",
      "    #word = stem(word)\n",
      "    word = strip(lemmatize(word), True)\n",
      "    return word\n",
      "\n",
      "def acceptable_word(word):\n",
      "    '''\n",
      "    Checks conditions for acceptable word: valid length and no stopwords\n",
      "    '''\n",
      "    accepted = bool(2 <= len(word) <= 40\n",
      "        and word.lower() not in sw)\n",
      "    return accepted\n",
      "\n",
      "def get_terms(tree):\n",
      "    '''\n",
      "    Gets all the acceptable noun_phrase term from the syntax tree\n",
      "    '''\n",
      "    for leaf in leaves(tree):\n",
      "        term = [normalize(w) for w, t in leaf if acceptable_word(w)]\n",
      "        yield term\n",
      "\n",
      "def extract_noun_phrases(text):\n",
      "    '''\n",
      "    Extracts all noun_phrases from a given text\n",
      "    '''\n",
      "    toks = nltk.regexp_tokenize(text, sentence_re)\n",
      "    postoks = tagger.tag(toks)\n",
      "\n",
      "    # Builds a POS tree\n",
      "    tree = chunker.parse(postoks)\n",
      "    terms = get_terms(tree)\n",
      "\n",
      "    # Extracts Noun Phrase\n",
      "    noun_phrases = []\n",
      "    for term in terms:\n",
      "        np = \"\"\n",
      "        for word in term:\n",
      "            np += word + \" \"\n",
      "        if np != \"\":\n",
      "            noun_phrases.append(np.strip())\n",
      "    return noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully loaded POS tagger\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feed(feed):\n",
      "    '''\n",
      "    Extracts popular topics (noun phrases) from a feed, and builds a simple\n",
      "    counter to keep track of the popularity\n",
      "    '''\n",
      "    topics = defaultdict(int)\n",
      "    for post, link in feed:\n",
      "        noun_phrases = extract_noun_phrases(post)\n",
      "        for np in noun_phrases:\n",
      "            if np != '':\n",
      "                topics[np] += 1\n",
      "    return topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topics = extract_feed(feed)\n",
      "c = Counter(topics)\n",
      "c.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "[('http', 89),\n",
        " ('class', 65),\n",
        " ('www', 63),\n",
        " ('semester', 63),\n",
        " ('c', 52),\n",
        " ('facebook', 41),\n",
        " ('phase', 40),\n",
        " ('ve', 38),\n",
        " ('course', 33),\n",
        " ('student', 30),\n",
        " ('guy', 29),\n",
        " ('re', 27),\n",
        " ('thanks', 26),\n",
        " ('61b', 25),\n",
        " ('math', 24),\n",
        " ('ee20', 21),\n",
        " ('people', 20),\n",
        " ('cs61b', 20),\n",
        " ('telebears', 19),\n",
        " ('question', 18)]"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pytagcloud import create_tag_image, create_html_data, make_tags, LAYOUT_HORIZONTAL, LAYOUTS\n",
      "from pytagcloud.colors import COLOR_SCHEMES\n",
      "from operator import itemgetter\n",
      "\n",
      "def get_tag_counts(counter):\n",
      "    '''\n",
      "    Get the noun phrase counts for word cloud by first converting the counter to a dict\n",
      "    '''\n",
      "    return sorted(dict(counter).iteritems(), key=itemgetter(1), reverse=True)\n",
      "    \n",
      "def create_cloud(counter, filename):\n",
      "    '''\n",
      "    Creates a word cloud from a counter\n",
      "    '''\n",
      "    tags = make_tags(get_tag_counts(counter)[:80], maxsize=120, \n",
      "                     colors=COLOR_SCHEMES['goldfish'])\n",
      "    create_tag_image(tags, './img/' + filename + '.png', \n",
      "                     size=(900, 600), background=(0, 0, 0, 255), \n",
      "                     layout=LAYOUT_HORIZONTAL, fontname='Lobster')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_cloud(c, 'cloud_large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/cloud_large.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Filtering out noises and see what we get! "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word in [\"http\", \"www\", \"facebook\"]:\n",
      "    topics[word] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = Counter(topics)\n",
      "c.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "[('class', 65),\n",
        " ('semester', 63),\n",
        " ('c', 52),\n",
        " ('phase', 40),\n",
        " ('ve', 38),\n",
        " ('course', 33),\n",
        " ('student', 30),\n",
        " ('guy', 29),\n",
        " ('re', 27),\n",
        " ('thanks', 26),\n",
        " ('61b', 25),\n",
        " ('math', 24),\n",
        " ('ee20', 21),\n",
        " ('people', 20),\n",
        " ('cs61b', 20),\n",
        " ('telebears', 19),\n",
        " ('question', 18),\n",
        " ('year', 17),\n",
        " ('61c', 17),\n",
        " ('waitlist', 17)]"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_cloud(c, 'cloud_large_1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](files/img/cloud_large_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenges:\n",
      "\n",
      "* Graph API's not-very-great documentation\n",
      "* FB Access Token expires every two hours...\n",
      "* FB limits the number of posts to be max 500. This may lead to inaccurate search information, especially when there are for sure more than 500 posts in the Berkeley CS FB group. Solution => include pagination to fetch everything in the future..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Results:\n",
      "\n",
      "Part I:\n",
      "\n",
      "* Tried out a few different queries with the simple TFIDF search, and the result seems to be on par with FB Search. The ranking, as seen above, is different, however. More are discussed in the final report.     \n",
      "\n",
      "* Revisit and try out different algorithms, including stemming, spelling correction, variants of TFIDF (maxmimum TF normalization, sublinear scaling, etc.) http://nlp.stanford.edu/IR-book/html/htmledition/variant-tf-idf-functions-1.html\n",
      "\n",
      "Part II:\n",
      "\n",
      "* There are quite some noises ('http', 'www', etc.) and words that don't really tell you anything new ('class', 'course', etc.). Is there a better way to filter them out instead of just hardcoding them?    \n",
      "\n",
      "* Perhaps we can combine Part I and II together, i.e. filter out the words that appear a lot across the document (high IDF score)?\n",
      "\n",
      "* The overall result seems consistent with what most CS students usually discuss on FB ('telebears', '61B', etc.)\n",
      "\n",
      "\n",
      "Overall a great project to work on and extend in the future!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Thanks!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}